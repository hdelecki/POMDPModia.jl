{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POMDPModia.jl: Getting Started\n",
    "\n",
    "The POMDPModia.jl package fully integrates with most other packages in the JuliaPOMDP ecosystem, including POMDPs.jl, POMDPPolicies.jl, BeliefUpdaters.jl, POMDPSimulators.jl, online and offline solvers.\n",
    "\n",
    "## Multiple tigers example\n",
    "\n",
    "In this notebook, a MODIA will be created for a example that has multiple amounts of different TigerPOMDPs behind doors. The objective is to pick the safest action, w.r.t. the individual beliefs tracked for each POMDP.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"../src/main.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using POMDPs\n",
    "using POMDPModels: TigerPOMDP\n",
    "using QMDP: QMDPSolver\n",
    "using Random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A MODIA object consists of five components:\n",
    "1. **Decision Problems (DPs)**: The minimum number of unique POMDPs required to effectively describe the problem at hand.\n",
    "2. **Decision Components (DCs)**: The amount of each DP.\n",
    "3. **Safety Sort Function (SSF)**: The function used to sort and pick the \"safest\" action amonng all the actions suggested by the policies of each individual POMDP.\n",
    "4. **Markov Processes (MPs)**: The entire stack of MDPs and/or POMDPs kept track, totalling to an amount of sum(DCs).\n",
    "5. **Beliefs**: Each individual belief kept track for each individual POMDP in MPs. \n",
    "\n",
    "In this example, we define three different tiger characteristics as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiger_problem1 = TigerPOMDP(-1.0, -100.0, 10.0, 0.90, 0.90);\n",
    "tiger_problem2 = TigerPOMDP(-2.0, -50.0, 17.0, 0.80, 0.60);\n",
    "tiger_problem3 = TigerPOMDP(-3.0, -75.0, 8.0, 0.85, 0.75);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first three components of MODIA can then be created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DPs = [tiger_problem1, tiger_problem2, tiger_problem3];\n",
    "DCs = [4, 1, 2];\n",
    "SSF = Base.minimum;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can instantiate a MODIA object, using one of the two methods below. Notice that we are initializing beliefs of markov processes through the BeliefUpdaters.jl package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Method 1 ##\n",
    "modia = MODIA(DPs, DCs, SSF)\n",
    "initialize_beliefs!(modia, BeliefUpdaters.uniform_belief);\n",
    "\n",
    "## Method 2 ##\n",
    "modia = MODIA(DPs, DCs, SSF, BeliefUpdaters.uniform_belief);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect this modia object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MODIA_of_POMDPs"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "typeof(modia)   # automatically determined that all MPs are POMDPS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Set{Int64} with 3 elements:\n",
       "  2\n",
       "  0\n",
       "  1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions(modia)  # set of all possible actions in MPs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7-element Array{TigerPOMDP,1}:\n",
       " TigerPOMDP(-1.0, -100.0, 10.0, 0.9, 0.9)\n",
       " TigerPOMDP(-1.0, -100.0, 10.0, 0.9, 0.9)\n",
       " TigerPOMDP(-1.0, -100.0, 10.0, 0.9, 0.9)\n",
       " TigerPOMDP(-1.0, -100.0, 10.0, 0.9, 0.9)\n",
       " TigerPOMDP(-2.0, -50.0, 17.0, 0.8, 0.6)\n",
       " TigerPOMDP(-3.0, -75.0, 8.0, 0.85, 0.75)\n",
       " TigerPOMDP(-3.0, -75.0, 8.0, 0.85, 0.75)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collect(modia)  # all MPs inside modia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7-element Array{DiscreteBelief{TigerPOMDP,Bool},1}:\n",
       " DiscreteBelief{TigerPOMDP,Bool}(TigerPOMDP(-1.0, -100.0, 10.0, 0.9, 0.9), Bool[0, 1], [0.5, 0.5])\n",
       " DiscreteBelief{TigerPOMDP,Bool}(TigerPOMDP(-1.0, -100.0, 10.0, 0.9, 0.9), Bool[0, 1], [0.5, 0.5])\n",
       " DiscreteBelief{TigerPOMDP,Bool}(TigerPOMDP(-1.0, -100.0, 10.0, 0.9, 0.9), Bool[0, 1], [0.5, 0.5])\n",
       " DiscreteBelief{TigerPOMDP,Bool}(TigerPOMDP(-1.0, -100.0, 10.0, 0.9, 0.9), Bool[0, 1], [0.5, 0.5])\n",
       " DiscreteBelief{TigerPOMDP,Bool}(TigerPOMDP(-2.0, -50.0, 17.0, 0.8, 0.6), Bool[0, 1], [0.5, 0.5])\n",
       " DiscreteBelief{TigerPOMDP,Bool}(TigerPOMDP(-3.0, -75.0, 8.0, 0.85, 0.75), Bool[0, 1], [0.5, 0.5])\n",
       " DiscreteBelief{TigerPOMDP,Bool}(TigerPOMDP(-3.0, -75.0, 8.0, 0.85, 0.75), Bool[0, 1], [0.5, 0.5])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collect_beliefs(modia)  # all beliefs inside modia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's choose an offline solver to compute optimal policies to each MP individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = QMDPSolver();\n",
    "policies = POMDPs.solve(solver, modia);  # includes 7 different policy, for each MP.\n",
    "act = safest_action(policies, modia);  # safest action, according to our SSF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define belief updaters, and receive and updated belief for some random observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bu = BeliefUpdaters.DiscreteUpdater(modia);    # can also use updater(policies) to retrieve an appropiate belief updater.\n",
    "random_obs = [Bool(rand(0:1)) for _ in modia.markov_prcs]  # create random observations for all POMDPs.\n",
    "new_belief = POMDPs.update(bu, modia, act, random_obs);   # calculates new belief, but does not update within modia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the belief of each POMDP has shifted, according to the random observations we have received:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7-element Array{DiscreteBelief{TigerPOMDP,Bool},1}:\n",
       " DiscreteBelief{TigerPOMDP,Bool}(TigerPOMDP(-1.0, -100.0, 10.0, 0.9, 0.9), Bool[0, 1], [0.09999999999999998, 0.9])\n",
       " DiscreteBelief{TigerPOMDP,Bool}(TigerPOMDP(-1.0, -100.0, 10.0, 0.9, 0.9), Bool[0, 1], [0.9, 0.09999999999999998])\n",
       " DiscreteBelief{TigerPOMDP,Bool}(TigerPOMDP(-1.0, -100.0, 10.0, 0.9, 0.9), Bool[0, 1], [0.9, 0.09999999999999998])\n",
       " DiscreteBelief{TigerPOMDP,Bool}(TigerPOMDP(-1.0, -100.0, 10.0, 0.9, 0.9), Bool[0, 1], [0.9, 0.09999999999999998])\n",
       " DiscreteBelief{TigerPOMDP,Bool}(TigerPOMDP(-2.0, -50.0, 17.0, 0.8, 0.6), Bool[0, 1], [0.8, 0.19999999999999996])\n",
       " DiscreteBelief{TigerPOMDP,Bool}(TigerPOMDP(-3.0, -75.0, 8.0, 0.85, 0.75), Bool[0, 1], [0.15000000000000002, 0.85])\n",
       " DiscreteBelief{TigerPOMDP,Bool}(TigerPOMDP(-3.0, -75.0, 8.0, 0.85, 0.75), Bool[0, 1], [0.85, 0.15000000000000002])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_belief.beliefs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To explicitly update the belief within a MODIA object, we can use the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7-element Array{DiscreteBelief{TigerPOMDP,Bool},1}:\n",
       " DiscreteBelief{TigerPOMDP,Bool}(TigerPOMDP(-1.0, -100.0, 10.0, 0.9, 0.9), Bool[0, 1], [0.09999999999999998, 0.9])\n",
       " DiscreteBelief{TigerPOMDP,Bool}(TigerPOMDP(-1.0, -100.0, 10.0, 0.9, 0.9), Bool[0, 1], [0.9, 0.09999999999999998])\n",
       " DiscreteBelief{TigerPOMDP,Bool}(TigerPOMDP(-1.0, -100.0, 10.0, 0.9, 0.9), Bool[0, 1], [0.9, 0.09999999999999998])\n",
       " DiscreteBelief{TigerPOMDP,Bool}(TigerPOMDP(-1.0, -100.0, 10.0, 0.9, 0.9), Bool[0, 1], [0.9, 0.09999999999999998])\n",
       " DiscreteBelief{TigerPOMDP,Bool}(TigerPOMDP(-2.0, -50.0, 17.0, 0.8, 0.6), Bool[0, 1], [0.8, 0.19999999999999996])\n",
       " DiscreteBelief{TigerPOMDP,Bool}(TigerPOMDP(-3.0, -75.0, 8.0, 0.85, 0.75), Bool[0, 1], [0.15000000000000002, 0.85])\n",
       " DiscreteBelief{TigerPOMDP,Bool}(TigerPOMDP(-3.0, -75.0, 8.0, 0.85, 0.75), Bool[0, 1], [0.85, 0.15000000000000002])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update!(bu, modia, act, random_obs);   # calculates new belief, and updates within modia\n",
    "collect_beliefs(modia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of keeping track of observations and belief updates by hand, we can simulate the MODIA for a desired number of steps, and received the discounted reward at the horizon. To do so, we use the following inputs:\n",
    "1. **Simulator properties**: An object to prescribe properties such as RNG, max steps, etc.\n",
    "2. **The MODIA object**: A MODIA instantiated using DPs, DCs, SSF, and initial beliefs.\n",
    "3. **Policies (online or offline)**: Policies to compute optimal actions for each MP.\n",
    "4. **Belief updater**: Belief updater object used to update POMDP beliefs are new observations are received.\n",
    "5. **Initial belief**: Beliefs to initialize simulation (defaults to modia.beliefs).\n",
    "6. **Initial states**: States to initialize the simulation.\n",
    "7. **MODIA Modification Function (MMF)**: [Optional] A function that specifies when new DPs or DCs are instantiated or terminated within MODIA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7-element Array{Float64,1}:\n",
       "   4.814900000000001\n",
       "   4.814900000000001\n",
       "   4.814900000000001\n",
       " -84.2851\n",
       "   2.2288\n",
       " -49.65234375\n",
       "  -2.96484375"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim = POMDPSimulators.RolloutSimulator(max_steps=5)\n",
    "initial_beliefs = modia.beliefs\n",
    "initial_states = convert(Array{Bool},(rand(initialstate(modia))))\n",
    "r_totals = POMDPs.simulate(sim, modia, policies, bu, initial_beliefs, initial_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The returned result above is the total discounted rewards for the specified 5 steps in the horizon. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define our custom MMF using the following functions to add/remove DPs and DCs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiger_problem4 = TigerPOMDP(-4.0, -80.0, 3.0, 0.55, 0.35);   # a new DP.\n",
    "add_DP!(modia, tiger_problem4);\n",
    "add_DCs!(modia, 4, 6, BeliefUpdaters.uniform_belief);  # DP: 4, DCs: 6\n",
    "remove_DPs!(modia, [1,2])\n",
    "remove_DC!(modia, 1, 1)  # DP: 1, DC: 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These operations result in the following DPs and DCs in our MODIA object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{TigerPOMDP,1}:\n",
       " TigerPOMDP(-3.0, -75.0, 8.0, 0.85, 0.75)\n",
       " TigerPOMDP(-4.0, -80.0, 3.0, 0.55, 0.35)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modia.DPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Int64,1}:\n",
       " 1\n",
       " 6"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modia.DCs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.2",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
